{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe7b8461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#函数库\n",
    "import time\n",
    "import gc\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b304ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "key1=np.array([\n",
    "0xb4 ,0x8e ,0xf3 ,0x52 ,0xba ,0x98 ,0x13 ,0x4e ,0x7f ,0x4d ,0x59 ,0x20 ,0x86 ,0x26 ,0x18 ,0x76\n",
    "])\n",
    "anum=[i for i in range(256)]\n",
    "ine=[0,5,10,15,4,9,14,3,8,13,2,7,12,1,6,11]\n",
    "random.seed(124)\n",
    "rands=np.zeros([160,256],dtype=np.int32)\n",
    "rands2=np.zeros([160,256],dtype=np.int32)\n",
    "for i in range(160):\n",
    "    rands[i][0:256]=random.sample(anum,256)[0:256]\n",
    "for i in range(160):\n",
    "    rands2[i][0:256]=random.sample(anum,256)[0:256]\n",
    "skey=random.sample(anum,16)\n",
    "\n",
    "ivsrands=np.zeros([160,256],dtype=np.int32)\n",
    "ivsrands2=np.zeros([160,256],dtype=np.int32)\n",
    "for k in range(10):\n",
    "    for i in range(16):\n",
    "        for j in range(256):\n",
    "            ivsrands[16*k+i][rands[ine[i]+16*k][j]]=j\n",
    "for k in range(10):\n",
    "    for i in range(16):\n",
    "        for j in range(256):\n",
    "            ivsrands2[16*k+i][rands2[ine[i]+16*k][j]]=j\n",
    "    #print(rands[i]) \n",
    "#rand1.sort() 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d330a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#参数\n",
    "lr=0.01    #学习率\n",
    "gamma=0   #动量法\n",
    "epochs=1000    #代数\n",
    "bs=8*256      #小批量数\n",
    "\n",
    "#输入数据集  (net.no,mingw)\n",
    "num=16\n",
    "a=256*num\n",
    "x=torch.zeros([a,1,16*32],dtype=torch.float32)\n",
    "#赋予网络号标签\n",
    "for i in range(a):\n",
    "    x[i,0,int(i/256)]=1    \n",
    "#赋明文\n",
    "for i in range(a):\n",
    "    x[i,0,256+i%256]=1\n",
    "#print(x[a-3])  #1\n",
    "x=x.reshape([a,1,16,32])\n",
    "\n",
    "\n",
    "#目标数据 （miwen)\n",
    "#赋初值\n",
    "y=torch.zeros([a],dtype=torch.int64)\n",
    "for i in range(num):\n",
    "    for j in range(256):\n",
    "        y[i*256+j]=j\n",
    "#print(y[0:512]) #1\n",
    "#ivsbyte2\n",
    "for i in range(a):\n",
    "    y[i]=ivsrands2[16*9+int(i/256)][y[i]]\n",
    "#ivsbyte\n",
    "for i in range(a):\n",
    "    y[i]=ivsrands[16*9+int(i/256)][y[i]]\n",
    "#exor\n",
    "for i in range(a):\n",
    "    y[i]=y[i]^(key1[int(i/256)])\n",
    "    \n",
    "device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#print(sys.getsizeof(x.storage()))\n",
    "#print(sys.getsizeof(y.storage()))\n",
    "dataset=TensorDataset(x,y)\n",
    "#print(dataset.data[0])\n",
    "data=DataLoader(dataset\n",
    "               ,batch_size=bs\n",
    "               ,drop_last=False)\n",
    "#for i in data:\n",
    "#    print(i[0].shape)\n",
    "finsig=torch.zeros([a,256],dtype=torch.float32)\n",
    "finsig=finsig.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2680f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#网络构建\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(1,16,3,padding=2)    #卷积\n",
    "        self.pool1=nn.MaxPool2d(2)  #最大池化\n",
    "        self.conv2=nn.Conv2d(16,32,3,padding=2)   #卷积\n",
    "        self.pool2=nn.MaxPool2d(2)            #最大池化\n",
    "        #self.conv3=nn.Conv2d(32,48,3)   #卷积\n",
    "        #self.pool3=nn.MaxPool2d(2)            #最大池化\n",
    "        \n",
    "        self.linear1=nn.Linear(5*9*32,2024,bias=True)  #两层全连接\n",
    "        self.linear2=nn.Linear(2024,256,bias=False)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=torch.tanh(self.conv1(x))   #前向传播 包括激活函数\n",
    "        x=self.pool1(x)\n",
    "        x=torch.tanh(self.conv2(x))\n",
    "        x=self.pool2(x)\n",
    "        #x=torch.tanh(self.conv3(x))\n",
    "        #x=self.pool3(x)\n",
    "        \n",
    "        x=x.view(-1,5*9*32)\n",
    "        x=torch.tanh(self.linear1(x))\n",
    "        output=F.softmax(self.linear2(x),dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49caedd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义损失函数以及优化算法及迭代\n",
    "def train(net,lr):\n",
    "    start = time.time()\n",
    "    #MSE函数\n",
    "    #criterion=nn.MSELoss()\n",
    "    criterion=nn.NLLLoss() #定义多分类损失函数\n",
    "    telr=lr\n",
    "    #opt=optim.SGD(net.parameters(),lr=telr,momentum=gamma) #小批量优化算法\n",
    "    opt=optim.Adam(net.parameters(),lr=telr,betas=(0.15,0.15),eps=1e-08,amsgrad=True)\n",
    "    for epoch in range(epochs):\n",
    "        num1=0\n",
    "        for m,n in data:\n",
    "            x1=m.to(device,non_blocking=True)\n",
    "            y1=n.to(device,non_blocking=True)\n",
    "            sigma=net.forward(x1)       #前向传播\n",
    "            sigma1=torch.log(sigma)    #softmax+log+NLLLOSS=多分类交叉熵函数\n",
    "            loss=criterion(sigma1,y1)   #计算损失函数值\n",
    "            loss.backward()            #反向传播\n",
    "            opt.step()                 #权重更新\n",
    "            opt.zero_grad()            #梯度清零\n",
    "            #telr=lr-lr*(1-(torch.pow(torch.tanh(torch.tensor(-10+10*(epoch/epochs))),2)))+0.1  #学习率衰减函数\n",
    "           \n",
    "            if epoch%50==0 and num1==0:\n",
    "                print('epoch:{} 该epochs中首个loss值:{} '.format(epoch,loss))\n",
    "            \n",
    "            if epoch==epochs-1:\n",
    "                    finsig[num1*bs:num1*bs+bs]=sigma[0:bs]\n",
    "            del sigma,sigma1,loss,x1,y1\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            num1=num1+1\n",
    "    #统计结果正确的标签数量\n",
    "    num=0\n",
    "    fina=torch.max(finsig,1)[1]  #每个长度为256的标签中的最大值所在的索引就是目标字节值，这是训练结果的值\n",
    "    #y1=torch.max(y,1)[1]        #目标值\n",
    "    for i in range(a):       \n",
    "        temp=fina[i]-y[i]\n",
    "        if temp==0:\n",
    "            num=num+1\n",
    "    print('映射成功的个数:%d'%num)\n",
    "    end = time.time()\n",
    "    print('程序执行时间: ',end - start)\n",
    "    return finsig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a15c8f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型实例化和确定随机数种子\n",
    "torch.manual_seed(126)\n",
    "#torch.cuda.manual_seed(126)\n",
    "torch.cuda.manual_seed_all(126)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "net=Model()\n",
    "net=net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8a81a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 该epochs中首个loss值:5.545862674713135 \n",
      "epoch:50 该epochs中首个loss值:4.9008660316467285 \n",
      "epoch:100 该epochs中首个loss值:4.020129680633545 \n",
      "epoch:150 该epochs中首个loss值:2.2016570568084717 \n",
      "epoch:200 该epochs中首个loss值:0.6652554273605347 \n",
      "epoch:250 该epochs中首个loss值:0.18929560482501984 \n",
      "epoch:300 该epochs中首个loss值:0.09330213069915771 \n",
      "epoch:350 该epochs中首个loss值:0.057596031576395035 \n",
      "epoch:400 该epochs中首个loss值:0.04034176096320152 \n",
      "epoch:450 该epochs中首个loss值:0.030532309785485268 \n",
      "epoch:500 该epochs中首个loss值:0.02431487664580345 \n",
      "epoch:550 该epochs中首个loss值:0.020066626369953156 \n",
      "epoch:600 该epochs中首个loss值:0.017001722007989883 \n",
      "epoch:650 该epochs中首个loss值:0.014699503779411316 \n",
      "epoch:700 该epochs中首个loss值:0.012913434766232967 \n",
      "epoch:750 该epochs中首个loss值:0.011486873030662537 \n",
      "epoch:800 该epochs中首个loss值:0.0103289894759655 \n",
      "epoch:850 该epochs中首个loss值:0.0093689588829875 \n",
      "epoch:900 该epochs中首个loss值:0.008563202805817127 \n",
      "epoch:950 该epochs中首个loss值:0.007877462543547153 \n",
      "映射成功的个数:4096\n",
      "程序执行时间:  92.88909769058228\n"
     ]
    }
   ],
   "source": [
    "#训练与评估\n",
    "sigma=train(net,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cec71ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.min(\n",
       "values=tensor(0.9531, grad_fn=<MinBackward0>),\n",
       "indices=tensor(3653))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#求出上述网络中误差最大的标签的值\n",
    "temp=[0 for i in range(a)]\n",
    "temp=torch.tensor(temp,dtype=torch.float32)\n",
    "for i in range(a):\n",
    "    temp[i]=torch.max(sigma[i],dim=0)[0]\n",
    "torch.min(temp,dim=0)  #value就是误差最大的值，ind就是对应值的索引号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "210906f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存模型\n",
    "torch.save(net,'net3v2.1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cbe1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
