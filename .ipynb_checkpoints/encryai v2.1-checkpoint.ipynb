{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ccecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入相关函数库\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f854aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入训练好的模型\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(1,16,3,padding=2)    #卷积\n",
    "        self.pool1=nn.MaxPool2d(2)  #最大池化\n",
    "        self.conv2=nn.Conv2d(16,32,3,padding=2)   #卷积\n",
    "        self.pool2=nn.MaxPool2d(2)            #最大池化\n",
    "        \n",
    "        self.linear1=nn.Linear(5*13*32,2024,bias=True)  #两层全连接\n",
    "        self.linear2=nn.Linear(2024,256,bias=False)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=torch.tanh(self.conv1(x))   #前向传播 包括激活函数\n",
    "        x=self.pool1(x)\n",
    "        x=torch.tanh(self.conv2(x))\n",
    "        x=self.pool2(x)\n",
    "        \n",
    "        x=x.view(-1,5*13*32)\n",
    "        x=torch.tanh(self.linear1(x))\n",
    "        output=F.softmax(self.linear2(x),dim=1)\n",
    "        return output\n",
    "\n",
    "net1=torch.load('net1v2.1.pkl')  \n",
    "net2=torch.load('net2v2.1.pkl')\n",
    "\n",
    "\n",
    "#导入训练好的模型\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(1,16,3,padding=2)    #卷积\n",
    "        self.pool1=nn.MaxPool2d(2)  #最大池化\n",
    "        self.conv2=nn.Conv2d(16,32,3,padding=2)   #卷积\n",
    "        self.pool2=nn.MaxPool2d(2)            #最大池化\n",
    "        \n",
    "        self.linear1=nn.Linear(5*9*32,2024,bias=True)  #两层全连接\n",
    "        self.linear2=nn.Linear(2024,256,bias=False)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=torch.tanh(self.conv1(x))   #前向传播 包括激活函数\n",
    "        x=self.pool1(x)\n",
    "        x=torch.tanh(self.conv2(x))\n",
    "        x=self.pool2(x)\n",
    "        \n",
    "        x=x.view(-1,5*9*32)\n",
    "        x=torch.tanh(self.linear1(x))\n",
    "        output=F.softmax(self.linear2(x),dim=1)\n",
    "        return output\n",
    "\n",
    "net3=torch.load('net3v2.1.pkl')\n",
    "xor=torch.load('xornet.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75d1be12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入待加密明文：\n",
      "0123456789abcdeffedcba9876543210\n"
     ]
    }
   ],
   "source": [
    "#输入待加密明文\n",
    "print(\"请输入待加密明文：\") #明文0123456789abcdeffedcba9876543210  默认密钥0f1571c947d9e8590cb7add6af7f6798\n",
    "a=input('')\n",
    "lenth=int(len(a)/2)\n",
    "if(lenth!=16):\n",
    "   print('输入长度不为16字节，请重新输入')\n",
    "temp=[0 for i in range(lenth)]  #临时保存整形明文数组\n",
    "for i in range(lenth):\n",
    "    temp[i]=int(a[i*2:i*2+2],16)\n",
    "#len(mingw) 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a28625c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1轮： 0xb9 0xe4 0x47 0xc5 0x94 0x8e 0x20 0xd6 0x57 0x16 0x9a 0xf5 0x75 0x51 0x3f 0x3b \n",
      "第2轮： 0x8e 0xb2 0xdf 0x2d 0x22 0xf2 0x80 0xc5 0xdb 0xdc 0xf7 0x1e 0x12 0x92 0xc1 0x52 \n",
      "第3轮： 0xb1 0xba 0xf9 0x1d 0xc1 0xf3 0x1f 0x19 0xb 0x8b 0x6a 0x24 0xcc 0x7 0xc3 0x5c \n",
      "第4轮： 0xd4 0x3b 0xcb 0x19 0x11 0x44 0xab 0xb7 0xfe 0x6 0x62 0x7 0xf 0x73 0x37 0xec \n",
      "第5轮： 0x2a 0x83 0x84 0xeb 0x47 0xe8 0x18 0x10 0xc4 0x18 0x27 0xa 0x48 0xba 0x23 0xf3 \n",
      "第6轮： 0x7b 0x1e 0x94 0x94 0x5 0xd0 0x83 0xc4 0x42 0x20 0x18 0x43 0x4a 0x40 0x52 0xfb \n",
      "第7轮： 0xec 0xc 0x3b 0xb7 0x1a 0x50 0xd7 0x22 0xc0 0x53 0x0 0x72 0x80 0xc7 0xef 0xe0 \n",
      "第8轮： 0xb1 0x3d 0xa 0x9f 0x1a 0x2f 0x6b 0x68 0x44 0xec 0x2f 0xf3 0x17 0xb6 0x42 0xb1 \n",
      "第9轮： 0x31 0xac 0x46 0x6a 0x30 0x71 0x65 0x1c 0x3a 0x8c 0x48 0x31 0xc2 0xc4 0xeb 0x62 \n",
      "第10轮： 0xff 0xb 0x84 0x4a 0x8 0x53 0xbf 0x7c 0x69 0x34 0xab 0x43 0x64 0x14 0x8f 0xb9 \n"
     ]
    }
   ],
   "source": [
    "#加密主体，10轮加密\n",
    "n=0\n",
    "while n<10:\n",
    "    #exor+byte+sbyte+sbyte2\n",
    "    mingw=torch.zeros([lenth,16*48],dtype=torch.float32) \n",
    "    for i in range(lenth):\n",
    "        mingw[i,256+16*n+i]=1\n",
    "        mingw[i,512+temp[i]]=1\n",
    "    plain=mingw.reshape([lenth,1,16,48])\n",
    "    plain=plain.cuda()\n",
    "    etemp=net1(plain)\n",
    "    #for i in range(lenth): #正确值 0x55 0x3b 0x81 0xd1 0x42 0x84 0xd9 0x8 0xff 0x26 0xc7 0xfb 0x2f 0xa4 0x19 0xd0 一致\n",
    "    #    print(hex(torch.max(etemp,dim=1)[1][i]),end=' ')\n",
    "    temp2=torch.max(etemp,dim=1)[1]\n",
    "\n",
    "    #行移位\n",
    "    temph=[0 for i in range(16)]\n",
    "    for i in range(4):\n",
    "        temph[i]=temp2[5*i]\n",
    "        temph[4+i]=temp2[(4+5*i)%16] \n",
    "        temph[8+i]=temp2[(8+5*i)%16] \n",
    "        temph[12+i]=temp2[(12+5*i)%16] \n",
    "    #for i in range(lenth): #0x55 0x84 0xc7 0xd0 0x42 0x26 0x19 0xd1 0xff 0xa4 0x81 0x8 0x2f 0x3b 0xd9 0xfb 一致\n",
    "    #    print(hex(temph[i]),end=' ')\n",
    "    #print('')\n",
    "\n",
    "    #列混淆\n",
    "    if n!=9:\n",
    "        index1=[0 for i in range(16)] #分解运算\n",
    "        index2=[0 for i in range(16)]\n",
    "        index3=[0 for i in range(16)]\n",
    "        index4=[0 for i in range(16)]\n",
    "        index1[0:16]=temph[0:16]\n",
    "        for i in range(4): #将同目的序列聚在一起\n",
    "            index2[4*i]=temph[4*i+1]\n",
    "            index2[4*i+1]=temph[4*i+2]\n",
    "            index2[4*i+2]=temph[4*i+3]\n",
    "            index2[4*i+3]=temph[4*i]\n",
    "            index3[4*i]=temph[4*i+2]\n",
    "            index3[4*i+1]=temph[4*i+3]\n",
    "            index3[4*i+2]=temph[4*i]\n",
    "            index3[4*i+3]=temph[4*i+1]\n",
    "            index4[4*i]=temph[4*i+3]\n",
    "            index4[4*i+1]=temph[4*i]\n",
    "            index4[4*i+2]=temph[4*i+1]\n",
    "            index4[4*i+3]=temph[4*i+2]\n",
    "        plain31=torch.zeros([lenth*4,16*48],dtype=torch.float32)\n",
    "        #ivsbyte2+ivsbyte+mul+skey\n",
    "        for i in range(lenth):     \n",
    "            plain31[i,0]=1  \n",
    "            plain31[i,256+16*n+i]=1\n",
    "            plain31[i,512+index1[i]]=1\n",
    "            plain31[16+i,1]=1\n",
    "            plain31[16+i,256+16*n+i]=1\n",
    "            plain31[16+i,512+index2[i]]=1\n",
    "            plain31[32+i,2]=1\n",
    "            plain31[32+i,256+16*n+i]=1\n",
    "            plain31[32+i,512+index3[i]]=1\n",
    "            plain31[48+i,3]=1\n",
    "            plain31[48+i,256+16*n+i]=1\n",
    "            plain31[48+i,512+index4[i]]=1            \n",
    "        plain31=plain31.reshape([lenth*4,1,16,48])\n",
    "        plain31=plain31.cuda()\n",
    "        tempby2=net2(plain31)\n",
    "        temp31=torch.max(tempby2[0:16],dim=1)[1]\n",
    "        temp32=torch.max(tempby2[16:32],dim=1)[1]\n",
    "        index3=torch.max(tempby2[32:48],dim=1)[1]\n",
    "        index4=torch.max(tempby2[48:64],dim=1)[1]       \n",
    "        #for i in range(lenth): #0x51 0x6b 0x2c 0x9b 0x84 0x3b 0xa3 0x22 0x68 0x3d 0x30 0x3 0xf9 0xac 0xf 0x70 \n",
    "        #    print(hex(temp31[i]),end=' ')\n",
    "        #print('')\n",
    "        #for i in range(lenth): #0xdc 0xe0 0x80 0xee 0x8 0xda 0x77 0x77 0x69 0xec 0xd2 0x1f 0x9c 0xe7 0x0 0x71\n",
    "        #    print(hex(temp32[i]),end=' ')\n",
    "        #print('') \n",
    "       \n",
    "        templ=torch.zeros([lenth*2,512],dtype=torch.float32)\n",
    "        for i in range(4): #左半异或\n",
    "            templ[4*i,temp31[4*i]]=1\n",
    "            templ[4*i,256+temp32[4*i]]=1\n",
    "            templ[4*i+1,index4[4*i+1]]=1\n",
    "            templ[4*i+1,256+temp31[4*i+1]]=1\n",
    "            templ[4*i+2,index3[4*i+2]]=1\n",
    "            templ[4*i+2,256+index4[4*i+2]]=1\n",
    "            templ[4*i+3,temp32[4*i+3]]=1\n",
    "            templ[4*i+3,256+index3[4*i+3]]=1    \n",
    "        for i in range(4): #右半异或\n",
    "            templ[16+4*i,index3[4*i]]=1\n",
    "            templ[16+4*i,256+index4[4*i]]=1\n",
    "            templ[16+4*i+1,temp32[4*i+1]]=1\n",
    "            templ[16+4*i+1,256+index3[4*i+1]]=1\n",
    "            templ[16+4*i+2,temp31[4*i+2]]=1\n",
    "            templ[16+4*i+2,256+temp32[4*i+2]]=1\n",
    "            templ[16+4*i+3,index4[4*i+3]]=1\n",
    "            templ[16+4*i+3,256+temp31[4*i+3]]=1  \n",
    "        templ=templ.reshape([lenth*2,1,16,32])\n",
    "        templ=templ.cuda()\n",
    "        texorl=xor(templ)\n",
    "        xorl=torch.max(texorl[0:16],dim=1)[1]\n",
    "        xorr=torch.max(texorl[16:32],dim=1)[1]\n",
    "        plain3=torch.zeros([lenth,512],dtype=torch.float32)\n",
    "        for i in range(4): #总异或\n",
    "            plain3[4*i,xorl[4*i]]=1\n",
    "            plain3[4*i,256+xorr[4*i]]=1\n",
    "            plain3[4*i+1,xorl[4*i+1]]=1\n",
    "            plain3[4*i+1,256+xorr[4*i+1]]=1\n",
    "            plain3[4*i+2,xorl[4*i+2]]=1\n",
    "            plain3[4*i+2,256+xorr[4*i+2]]=1\n",
    "            plain3[4*i+3,xorl[4*i+3]]=1\n",
    "            plain3[4*i+3,256+xorr[4*i+3]]=1  \n",
    "        plain3=plain3.reshape([lenth,1,16,32])\n",
    "        plain3=plain3.cuda()\n",
    "        xtemp=xor(plain3)\n",
    "        temp=torch.max(xtemp,dim=1)[1]\n",
    "        print('第%d轮：'%(n+1),end=' ')\n",
    "        for i in range(lenth): #0xb9 0xe4 0x47 0xc5 0x94 0x8e 0x20 0xd6 0x57 0x16 0x9a 0xf5 0x75 0x51 0x3f 0x3b 一致\n",
    "            print(hex(temp[i]),end=' ')\n",
    "        print('')   \n",
    "    if n==9:\n",
    "        #ivsbyte2+ivsbyte+最后的exor\n",
    "        plain4=torch.zeros([lenth,512],dtype=torch.float32) \n",
    "        for i in range(lenth):\n",
    "            plain4[i,i]=1\n",
    "            plain4[i,256+temph[i]]=1\n",
    "        plain4=plain4.reshape([lenth,1,16,32])\n",
    "        plain4=plain4.cuda()\n",
    "        etemp=net3(plain4)\n",
    "        temp4=torch.max(etemp,dim=1)[1]\n",
    "        print('第10轮：',end=' ')\n",
    "        for i in range(lenth): \n",
    "            print(hex(temp4[i]),end=' ')\n",
    "        print('') \n",
    "    n=n+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db79c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3014d00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
